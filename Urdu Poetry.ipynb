{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing The Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import heapq\n",
    "from operator import itemgetter\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading The Poetry Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"E:/FAST_UNI/3rd_Semester/NLP/Assignments/2nd/poetry.txt\"\n",
    "\n",
    "with open(file_path , encoding = \"utf-8\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the corpus to see its structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and cleaning the corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[]\n",
    "for line in lines:\n",
    "        if not line.isspace():\n",
    "            line = line.strip()\n",
    "            line = line.replace('.','').replace(',','').replace(':','').replace(';','').replace('!','').replace('?','').replace('(',' ').replace(')',' ').replace(\"-\",\" \").replace(\"' \",'').replace('\"','').replace(\"_\",\"\")\n",
    "            token = line.split(' ')\n",
    "            tokens += token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token.strip('’’') for token in tokens]\n",
    "tokens = [token.strip(' ') for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uni-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a dictionary for Uni-Gram Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_grams_counts = dict({})\n",
    "\n",
    "for word in tokens:\n",
    "    if word not in uni_grams_counts:\n",
    "        uni_grams_counts[word] = 1\n",
    "    else:\n",
    "        uni_grams_counts[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a dictionary for Uni-Gram Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_grams = dict({})\n",
    "valuesss = sum(uni_grams_counts.values())\n",
    "for word in uni_grams_counts:\n",
    "    word_count = uni_grams_counts.get(word) # One Word Count\n",
    "    word_probability = word_count / valuesss\n",
    "    uni_grams[word] = word_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for Picking the first random word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickWord():\n",
    "    while(True):\n",
    "        top_frquent_words = heapq.nlargest(20, uni_grams_counts.items(), key=itemgetter(1))\n",
    "        rand = random.randint(0,19)\n",
    "        \n",
    "        try:\n",
    "            word = top_frquent_words[rand][0]\n",
    "            return word\n",
    "        except:\n",
    "            print(rand)\n",
    "            print(\"Caught Exception\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for printing the whole Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printStanza(stanza):\n",
    "    for i in range(0,12): # 1 - 12\n",
    "        if i % 4 == 0 and i != 0:\n",
    "            print(\" \")\n",
    "            print(stanza[i])\n",
    "        else:\n",
    "            print(stanza[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a dictionary for Bi-Gram Counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_grams_counts = dict({})\n",
    "\n",
    "for i in range(len(tokens) - 1):\n",
    "    if tokens[i] not in bi_grams_counts:\n",
    "        bi_grams_counts[word] = 1\n",
    "    s = tokens[i] + \" \" + tokens[i + 1]\n",
    "    if s not in bi_grams_counts:\n",
    "        bi_grams_counts[s] = 1\n",
    "    else:\n",
    "        bi_grams_counts[s] +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a dictionary for Bi-Gram Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_grams = dict({})\n",
    "\n",
    "for word in bi_grams_counts:\n",
    "    word_count = bi_grams_counts.get(word) # One Word Count\n",
    "    word_uni_prob = word.split(\" \")\n",
    "    word_uni_prob = word_uni_prob[0]\n",
    "    word_probability = word_count / uni_grams_counts.get(word_uni_prob)\n",
    "    bi_grams[word] = word_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a single verse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_word = pickWord()\n",
    "\n",
    "identified_word = None\n",
    "identified_prob = 0\n",
    "\n",
    "verse = first_word\n",
    "\n",
    "for i in range(1,6):\n",
    "    for word in bi_grams:\n",
    "        tokenized_word = word.split(\" \")\n",
    "        if tokenized_word[0] == first_word:\n",
    "            if bi_grams.get(word) > identified_prob:\n",
    "                identified_word = word\n",
    "                identified_prob = bi_grams.get(word)\n",
    "    \n",
    "    if identified_word != None:\n",
    "        wordss = identified_word.split(\" \")\n",
    "        first_word = wordss[1]\n",
    "        verse = verse + \" \"+ first_word\n",
    "        identified_prob = 0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'میں ہے کہ  ہم نے'"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a full Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_word = \"\"\n",
    "\n",
    "identified_word = None\n",
    "identified_prob = 0\n",
    "stanza = []\n",
    "\n",
    "verse = \"\"\n",
    "\n",
    "rand = random.randint(3,7)\n",
    "for j in range(0,3):\n",
    "    for j in range(0,4):\n",
    "        first_word = pickWord()\n",
    "        verse = verse + \" \" + first_word\n",
    "        for i in range(1,rand):\n",
    "            for word in bi_grams:\n",
    "                tokenized_word = word.split(\" \")\n",
    "                if tokenized_word[0] == first_word:\n",
    "                    if bi_grams.get(word) > identified_prob:\n",
    "                        identified_word = word\n",
    "                        identified_prob = bi_grams.get(word)\n",
    "\n",
    "            wordss = identified_word.split(\" \")\n",
    "            first_word = wordss[1]\n",
    "            verse = verse + \" \"+ first_word\n",
    "            identified_prob = 0\n",
    "        stanza.append(verse)\n",
    "        first_word = pickWord()\n",
    "        verse = first_word\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ہوں میں ہے کہ  ہم نے\n",
      "و نہ ہوا ہے کہ  ہم نے\n",
      "وہ نہ ہوا ہے کہ  ہم نے\n",
      "نہ ہم نے چاہا تھا کہ  ہم\n",
      " \n",
      "کا میں ہے کہ  ہم نے چاہا\n",
      "ہے و دل کا نہ ہوا ہے کہ\n",
      "ہے سے ہم نے چاہا تھا کہ \n",
      "میں کہ  ہم نے چاہا تھا کہ\n",
      " \n",
      "کی و دل کا نہ ہوا ہے کہ\n",
      "نہیں کے لیے ہم نے چاہا تھا کہ\n",
      "کے تھا کہ  ہم نے چاہا تھا\n",
      "تو تھا کہ  ہم نے چاہا تھا\n"
     ]
    }
   ],
   "source": [
    "printStanza(stanza)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Bi-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Bi-Gram Counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_bigram_model_counts = dict({})\n",
    "\n",
    "def reverseWord(word):\n",
    "    words = word.split(\" \")\n",
    "    if len(words) > 1:\n",
    "        newword = words[1] + \" \" + words[0]\n",
    "        return newword\n",
    "\n",
    "\n",
    "for i in range(len(tokens) - 1):\n",
    "    neword = reverseWord(tokens[i])\n",
    "    if neword not in backward_bigram_model_counts:\n",
    "        backward_bigram_model_counts[neword] = 1\n",
    "    s = tokens[i + 1] + \" \" + tokens[i]\n",
    "    if s not in backward_bigram_model_counts:\n",
    "        backward_bigram_model_counts[s] = 1\n",
    "    else:\n",
    "        backward_bigram_model_counts[s] +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Bi-Gram Probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_bi_grams = dict({})\n",
    "\n",
    "for word in backward_bigram_model_counts:\n",
    "    if word != None:\n",
    "        word_count = backward_bigram_model_counts.get(word) # One Word Count\n",
    "        word_uni_prob = word.split(\" \")\n",
    "        word_uni_prob = word_uni_prob[0]\n",
    "        word_probability = word_count / uni_grams_counts.get(word_uni_prob)\n",
    "        backward_bi_grams[word] = word_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a single verse with Backward Bi-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_word = \"گر\"\n",
    "\n",
    "identified_word = None\n",
    "identified_prob = 0\n",
    "\n",
    "verse = first_word\n",
    "\n",
    "for i in range(1,6):\n",
    "    for word in backward_bi_grams:\n",
    "        tokenized_word = word.split(\" \")\n",
    "        if tokenized_word[0] == first_word:\n",
    "            if backward_bi_grams.get(word) > identified_prob:\n",
    "                identified_word = word\n",
    "                identified_prob = backward_bi_grams.get(word)\n",
    "    \n",
    "    wordss = identified_word.split(\" \")\n",
    "    first_word = wordss[1]\n",
    "    verse = verse + \" \"+ first_word\n",
    "    identified_prob = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a Stanza with Backward Bi-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_word = \"\"\n",
    "\n",
    "identified_word = None\n",
    "identified_prob = 0\n",
    "stanza = []\n",
    "\n",
    "verse = \"\"\n",
    "\n",
    "rand = random.randint(3,7)\n",
    "for j in range(0,3):\n",
    "    for j in range(0,4):\n",
    "        first_word = pickWord()\n",
    "        verse = verse + \" \" + first_word\n",
    "        for i in range(1,rand):\n",
    "            for word in backward_bi_grams:\n",
    "                tokenized_word = word.split(\" \")\n",
    "                if tokenized_word[0] == first_word:\n",
    "                    if backward_bi_grams.get(word) > identified_prob:\n",
    "                        identified_word = word\n",
    "                        identified_prob = backward_bi_grams.get(word)\n",
    "\n",
    "            wordss = identified_word.split(\" \")\n",
    "            first_word = wordss[1]\n",
    "            verse = verse + \" \"+ first_word\n",
    "            identified_prob = 0\n",
    "        stanza.append(verse)\n",
    "        first_word = pickWord()\n",
    "        verse = first_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " کو اُس کہیں نہ کیوں ہی\n",
      "یہ دل و پیچ پر بات ایک\n",
      "وہ تو ہے کیا گے جائیں بڑھ\n",
      "و کے اس اب ہے کیا گے\n",
      " \n",
      "کی ہوں نہیں بھی میں ہوں نہیں\n",
      "میں کو اُس کہیں نہ کیوں ہی\n",
      "ہم کی اس اب ہے کیا گے\n",
      "وہ کہ ہے کیا گے جائیں بڑھ\n",
      " \n",
      "کی کہ ہے کیا گے جائیں بڑھ\n",
      "میں وہ ہے کیا گے جائیں بڑھ\n",
      "ہے کہ ہے کیا گے جائیں بڑھ\n",
      "تو کے اس اب ہے کیا گے\n"
     ]
    }
   ],
   "source": [
    "printStanza(stanza)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_word = \"ہوں\"\n",
    "verse = first_word\n",
    "\n",
    "identified_word_next = first_word\n",
    "identified_prob_next = 0\n",
    "\n",
    "identified_word_back = first_word\n",
    "identified_prob_back = 0\n",
    "\n",
    "for i in range(0,4): # 4\n",
    "    \n",
    "    for word in bi_grams:\n",
    "        tokenized_word = word.split(\" \")\n",
    "        if len(tokenized_word) > 1:\n",
    "            if tokenized_word[0] == identified_word_next:\n",
    "                if bi_grams.get(word) > identified_prob_next:\n",
    "                    identified_word_next = word.split(\" \")[1]\n",
    "                    identified_prob_next = bi_grams.get(word)\n",
    "    \n",
    "    verse = verse + \" \" + identified_word_next\n",
    "    identified_prob_next = 0\n",
    "    \n",
    "    for word in backward_bi_grams:\n",
    "        tokenized_word = word.split(\" \")\n",
    "        if len(tokenized_word) > 1:\n",
    "            if tokenized_word[0] == identified_word_back:\n",
    "                if backward_bi_grams.get(word) > identified_prob_back:\n",
    "                    identified_word_back = word.split(\" \")[1]\n",
    "                    identified_prob_back = backward_bi_grams.get(word)\n",
    "\n",
    "    verse = verse + \" \" + identified_word_back\n",
    "    identified_prob_back = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a verse with Bidirectional Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ہوں کی آرا پہ خود مرے و آگے شامت'"
      ]
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a stanza with bi-directional Bi-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_word = pickWord()\n",
    "verse = first_word\n",
    "\n",
    "identified_word_next = first_word\n",
    "identified_prob_next = 0\n",
    "\n",
    "identified_word_back = first_word\n",
    "identified_prob_back = 0\n",
    "\n",
    "list_of_poetry = []\n",
    "\n",
    "rand = random.randint(3,7)\n",
    "\n",
    "for j in range(0,3):\n",
    "\n",
    "    for i in range(0,4): # 4\n",
    "        for m in range(1,round(rand/2)):\n",
    "            for word in bi_grams:\n",
    "                tokenized_word = word.split(\" \")\n",
    "                if len(tokenized_word) > 1:\n",
    "                    if tokenized_word[0] == identified_word_next:\n",
    "                        if bi_grams.get(word) > identified_prob_next:\n",
    "                            identified_word_next = word.split(\" \")[1]\n",
    "                            identified_prob_next = bi_grams.get(word)\n",
    "\n",
    "            verse = verse + \" \" + identified_word_next\n",
    "            identified_word_next = pickWord()\n",
    "            identified_prob_next = 0\n",
    "\n",
    "            for word in backward_bi_grams:\n",
    "                tokenized_word = word.split(\" \")\n",
    "                if len(tokenized_word) > 1:\n",
    "                    if tokenized_word[0] == identified_word_back:\n",
    "                        if backward_bi_grams.get(word) > identified_prob_back:\n",
    "                            identified_word_back = word.split(\" \")[1]\n",
    "                            identified_prob_back = backward_bi_grams.get(word)\n",
    "\n",
    "            verse = verse + \" \" + identified_word_back\n",
    "            identified_word_back = pickWord()\n",
    "            identified_prob_back = 0\n",
    "\n",
    "        list_of_poetry.append(verse)\n",
    "        verse = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "وہ صورت قائل\n",
      " و مرے\n",
      " کون مرے\n",
      " ہے مرے\n",
      " \n",
      " ہے دل\n",
      " صورت اسدؔ\n",
      " گزرے اسدؔ\n",
      " گزرے میں\n",
      " \n",
      " راہ اسدؔ\n",
      " کون نہ\n",
      " کون دل\n",
      " مرے اسدؔ\n"
     ]
    }
   ],
   "source": [
    "printStanza(list_of_poetry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tri-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tri-Gram Count Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_grams_counts = dict({})\n",
    "\n",
    "for i in range(len(tokens) - 2):\n",
    "    s = tokens[i] + \" \" + tokens[i + 1] + \" \" + tokens[i + 2]\n",
    "    if s not in tri_grams_counts:\n",
    "        tri_grams_counts[s] = 1\n",
    "    else:\n",
    "        tri_grams_counts[s] +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tri-Gram Probabilities Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_grams = dict({})\n",
    "\n",
    "for word in tri_grams_counts:\n",
    "    word_count = tri_grams_counts.get(word) # One Word Count\n",
    "    word_uni_prob = word.split(\" \")\n",
    "    word_uni_prob = word_uni_prob[0] + \" \" + word_uni_prob[1]\n",
    "    \n",
    "    count = bi_grams_counts.get(word_uni_prob)\n",
    "    \n",
    "    if count != None:\n",
    "        word_probability = word_count / bi_grams_counts.get(word_uni_prob)\n",
    "        tri_grams[word] = word_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a single verse with Tri-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_word = pickWord()\n",
    "\n",
    "identified_word = None\n",
    "identified_prob = 0\n",
    "\n",
    "verse = first_word\n",
    "\n",
    "for i in range(1,6):\n",
    "    for word in tri_grams:\n",
    "        tokenized_word = word.split(\" \")\n",
    "        if tokenized_word[0] == first_word:\n",
    "            if tri_grams.get(word) > identified_prob:\n",
    "                identified_word = word\n",
    "                identified_prob = tri_grams.get(word)\n",
    "    \n",
    "    if identified_word != None:\n",
    "        wordss = identified_word.split(\" \")\n",
    "        first_word = wordss[1]\n",
    "        verse = verse + \" \"+ first_word\n",
    "        identified_prob = 0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'کے بغیر کون سے گزرا کیوں'"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a Stanza with Tr-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_word = pickWord()\n",
    "\n",
    "identified_word = None\n",
    "identified_prob = 0\n",
    "stanza = []\n",
    "\n",
    "verse = \"\"\n",
    "\n",
    "rand = random.randint(3,7)\n",
    "for j in range(0,3):\n",
    "    for j in range(0,4):\n",
    "        first_word = pickWord()\n",
    "        verse = verse + \" \" + first_word\n",
    "        for i in range(1,rand):\n",
    "            for word in tri_grams:\n",
    "                tokenized_word = word.split(\" \")\n",
    "                if tokenized_word[0] == first_word:\n",
    "                    if tri_grams.get(word) > identified_prob:\n",
    "                        identified_word = word\n",
    "                        identified_prob = tri_grams.get(word)\n",
    "\n",
    "            wordss = identified_word.split(\" \")\n",
    "            first_word = wordss[1]\n",
    "            verse = verse + \" \"+ first_word\n",
    "            identified_prob = 0\n",
    "        stanza.append(verse)\n",
    "        first_word = pickWord()\n",
    "        verse = first_word\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " سے گزرا کیوں ہوں\n",
      "کہ بھی جل گیا ہوگا\n",
      "کیا یہ حجاب پاس وضع\n",
      "وہ کا گماں گزرے ہے\n",
      " \n",
      "کہ یہ حجاب پاس وضع\n",
      "کو تو دیکھ لوں دو\n",
      "بھی دل عزیز اس طرۂ\n",
      "و کا گماں گزرے ہے\n",
      " \n",
      "نہیں تو دیکھ لوں دو\n",
      "کی تو دیکھ لوں دو\n",
      "یہ دل عزیز اس طرۂ\n",
      "بھی تھا عہد بودا کبھی\n"
     ]
    }
   ],
   "source": [
    "printStanza(stanza)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhyme_1 = [\"گفتگو\" , \"تندخو\" , \"رفو\" , \"جستجو\" , \"لہو\" , \"آرزو\"]\n",
    "rhyme_2 = [\"انتظار\" , \"اعتبار\" , \"استوار\" , \"غمگسار\" , \"روزگار\" , \"مزار\"]\n",
    "rhyme_3 = [\"گریباں\" , \"طوفاں\" , \"آساں\" , \"پریشاں\" , \"پرافشاں\" , \"بیاباں\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rand = random.randint(3,7)\n",
    "\n",
    "poetry = []\n",
    "\n",
    "first_word = pickWord()\n",
    "verse = first_word\n",
    "\n",
    "identified_word_next = first_word\n",
    "identified_prob_next = 0\n",
    "\n",
    "identified_word_back = first_word\n",
    "identified_prob_back = 0\n",
    "\n",
    "rhyme_rand = random.randint(1,4)\n",
    "\n",
    "for j in range(0,3):\n",
    "\n",
    "    for i in range(0,rand): # 4\n",
    "\n",
    "        for word in bi_grams:\n",
    "            tokenized_word = word.split(\" \")\n",
    "            if len(tokenized_word) > 1:\n",
    "                if tokenized_word[0] == identified_word_next:\n",
    "                    if bi_grams.get(word) > identified_prob_next:\n",
    "                        identified_word_next = word.split(\" \")[1]\n",
    "                        identified_prob_next = bi_grams.get(word)\n",
    "\n",
    "        verse = verse + \" \" + identified_word_next\n",
    "        identified_prob_next = 0\n",
    "\n",
    "        for word in backward_bi_grams: \n",
    "            tokenized_word = word.split(\" \")\n",
    "            if len(tokenized_word) > 1:\n",
    "                if tokenized_word[0] == identified_word_back:\n",
    "                    if backward_bi_grams.get(word) > identified_prob_back:\n",
    "                        identified_word_back = word.split(\" \")[1]\n",
    "                        identified_prob_back = backward_bi_grams.get(word)\n",
    "\n",
    "        verse = verse + \" \" + identified_word_back\n",
    "        identified_prob_back = 0\n",
    "    \n",
    "    if rhyme_rand == 0:\n",
    "        rhyme_rand_number = random.randint(0,5)\n",
    "        verse = verse + \" \" + rhyme_1[rhyme_rand_number]\n",
    "    elif rhyme_rand == 1:\n",
    "        rhyme_rand_number = random.randint(0,5)\n",
    "        verse = verse + \" \" + rhyme_2[rhyme_rand_number]\n",
    "    else:\n",
    "        rhyme_rand_number = random.randint(0,5)\n",
    "        verse = verse + \" \" + rhyme_3[rhyme_rand_number]\n",
    "    \n",
    "    poetry.append(verse)\n",
    "    verse = \"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['دل ہے گل آگے ہے ہے اطفال آگے بازیچۂ ہے بازیچۂ آگے بازیچۂ غمگسار',\n",
       " ' ہے بازیچۂ آگے بازیچۂ ہے بازیچۂ آگے بازیچۂ ہے بازیچۂ آگے بازیچۂ مزار',\n",
       " ' ہے بازیچۂ آگے بازیچۂ ہے بازیچۂ آگے بازیچۂ ہے بازیچۂ آگے بازیچۂ اعتبار']"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
